[{"authors":["admin"],"categories":null,"content":" Biography Georgios completed his undergraduate studies in the Mathematics department of the Aristotle University of Thessaloniki. After that, he joined the M.Sc. program in Computational Science at the Department for Theoretical Physics of Frankfurt University with a specialization on Machine Learning and Computational Neuroscience. His doctoral research was supported collaboratively by Prof. Dr. Roland Memisevic at the University of Frankfurt (2012-2013), Prof. Dr. Bruno Olshausen at the Redwood Center for Theoretical Neuroscience at UC Berkeley (2013-2014), and Prof. Dr. Jörg Lücke at the University of Oldenburg(2014-2016). As a result, he received the degree of Dr. rer. nat. from the University of Oldenburg on 01.11.2016. He worked as postdoctoral researcher at Prof. Dr. Stephane Mallat’s DATA team at École Normale Supérieure. Currently, he works at institut de la vision.\nHis research interests revolve around sparse coding, graphical models, deep learning, and theoretical neuroscience. He has studied their applications primarily on spike sorting, receptive field estimation, quantum chemistry, and natural image statistics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://gexarcha.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Biography Georgios completed his undergraduate studies in the Mathematics department of the Aristotle University of Thessaloniki. After that, he joined the M.Sc. program in Computational Science at the Department for Theoretical Physics of Frankfurt University with a specialization on Machine Learning and Computational Neuroscience. His doctoral research was supported collaboratively by Prof. Dr. Roland Memisevic at the University of Frankfurt (2012-2013), Prof. Dr. Bruno Olshausen at the Redwood Center for Theoretical Neuroscience at UC Berkeley (2013-2014), and Prof.","tags":null,"title":"Georgios Exarchakis","type":"author"},{"authors":["Georgios Exarchakis"],"categories":[],"content":"Kymatio is a Python module for computing wavelet and scattering transforms.\nIt is built on top of PyTorch, but also has a fast CUDA backend via cupy and skcuda.\nUse kymatio if you need a library that:\n integrates wavelet scattering in a deep learning architecture, supports 1-D, 2-D, and 3-D wavelets, and runs seamlessly on CPU and GPU hardware. A brief intro to wavelet scattering is provided in User Guide. For a list of publications see Publications.  Quick Start On Linux or macOS, open a shell and run the instruction of kymatio.\nIn the Python intepreter, you may then call:\nimport kymatio which should run without error if the package has been correctly installed.\nApply 2D scattering to a 32x32 random image The following code imports torch and the Scattering2D class, which implements the 2D scattering transform. It then creates an instance of this class to compute the scattering transform at scale J = 2 of a 32x32 image consisting of Gaussian white noise:\nimport torch from kymatio import Scattering2D scattering = Scattering2D(J=2, shape=(32, 32)) x = torch.randn(1, 1, 32, 32) Sx = scattering(x) print(Sx.size()) This should output:\ntorch.Size([1, 1, 81, 8, 8]) This corresponds to 81 scattering coefficients, each corresponding to an 8x8 image.\nCheck out the User Guide for more scattering transform examples.\n","date":1554205044,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554205044,"objectID":"4a33b4e600d8ddecb7ee98ab7ef4ced4","permalink":"https://gexarcha.github.io/post/kymatio/","publishdate":"2019-04-02T13:37:24+02:00","relpermalink":"/post/kymatio/","section":"post","summary":"Kymatio is a Python module for computing wavelet and scattering transforms.\nIt is built on top of PyTorch, but also has a fast CUDA backend via cupy and skcuda.\nUse kymatio if you need a library that:\n integrates wavelet scattering in a deep learning architecture, supports 1-D, 2-D, and 3-D wavelets, and runs seamlessly on CPU and GPU hardware. A brief intro to wavelet scattering is provided in User Guide.","tags":["wavelets"],"title":"Kymatio","type":"post"},{"authors":[],"categories":null,"content":"","date":1554204096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554204096,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"https://gexarcha.github.io/publication/example/","publishdate":"2019-04-02T13:21:36+02:00","relpermalink":"/publication/example/","section":"publication","summary":"","tags":[],"title":"Example","type":"publication"},{"authors":["Mathieu Andreux","Tomás Angles","Georgios Exarchakis","Roberto Leonarduzzi","Gaspar Rochette","Louis Thiry","John Zarka","Stéphane Mallat","Joakim Andén","Eugene Belilovsky","Joan Bruna","Vincent Lostanlen","Matthew J. Hirn","Edouard Oyallon","Sixhin Zhang","Carmine Cella","Michael Eickenberg"],"categories":null,"content":"","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"023581ec9f6248929dfdd2fffc2dfa12","permalink":"https://gexarcha.github.io/publication/kymatio-scattering-transforms-in-python/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/publication/kymatio-scattering-transforms-in-python/","section":"publication","summary":"The wavelet scattering transform is an invariant signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks. All transforms may be executed on a GPU (in addition to CPU), offering a considerable speed up over CPU implementations. The package also has a small memory footprint, resulting inefficient memory usage. The source code, documentation, and examples are available undera BSD license at this https URL","tags":["deep learning","wavelets","neural networks"],"title":"Kymatio: Scattering Transforms in Python","type":"publication"},{"authors":["Jörg Lücke","Zhenwen Dai","Georgios Exarchakis"],"categories":[],"content":"","date":1530362338,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530362338,"objectID":"647d455f2bf274d0d99078f8d0d53e71","permalink":"https://gexarcha.github.io/publication/truncated-variational-sampling-for-black-box-optimization-of-generative-models/","publishdate":"2018-06-30T14:38:58+02:00","relpermalink":"/publication/truncated-variational-sampling-for-black-box-optimization-of-generative-models/","section":"publication","summary":"We investigate the optimization of two probabilistic generative models with binary latent variables using a novel variational EM approach. The approach distinguishes itself from previous variational approaches by using latent states as variational parameters. Here we use efficient and general purpose sampling procedures to vary the latent states, and investigate the black box applicability of the resulting optimization procedure. For general purpose applicability, samples are drawn from approximate marginal distributions of the considered generative model as well as from the model’s prior distribution. As such, variational sampling is defined in a generic form, and is directly executable for a given model. As a proof of concept, we then apply the novel procedure (A) to Binary Sparse Coding (a model with continuous observables), and (B) to basic Sigmoid Belief Networks (which are models with binary observables). Numerical experiments verify that the investigated approach efficiently as well as effectively increases a variational free energy objective without requiring any additional analytical steps.","tags":["machine learning","graphical models","unsupervised learning","optimization"],"title":"Truncated Variational Sampling for ‘Black Box’ Optimization of Generative Models","type":"publication"},{"authors":["Michael Eickenberg","Georgios Exarchakis","Matthew Hirn","Stephane Mallat","Louis Thiry"],"categories":null,"content":"","date":1530144000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530144000,"objectID":"33d9b44412afb80d6e3b458308cebc25","permalink":"https://gexarcha.github.io/publication/solid-harmonic-wavelet-scattering-for-predictions-of-molecule-properties/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/publication/solid-harmonic-wavelet-scattering-for-predictions-of-molecule-properties/","section":"publication","summary":"We present a machine learning algorithm for the prediction of molecule properties inspired by ideas from density functional theory (DFT). Using Gaussian-type orbital functions, we create surrogate electronic densities of the molecule from which we compute invariant “solid harmonic scattering coefficients” that account for different types of interactions at different scales. Multilinear regressions of various physical properties of molecules are computed from these invariant coefficients. Numerical experiments show that these regressions have near state-of-the-art performance, even with relatively few training examples. Predictions  over small sets of scattering coefficients can reach a DFT precision while being  interpretable.","tags":["wavelets","quantum chemistry","multilinear regression","deep learning"],"title":"Solid harmonic wavelet scattering for predictions of molecule properties","type":"publication"},{"authors":["Michael Eickenberg","Georgios Exarchakis","Matthew Hirn","Stephane Mallat"],"categories":null,"content":"","date":1511222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511222400,"objectID":"bde31196d1c2890ff16956bf79545944","permalink":"https://gexarcha.github.io/publication/solid-harmonic-wavelet-scattering-predicting-quantum-molecular-energy-from-invariant-descriptors-of-3d-electronic-densities/","publishdate":"2017-11-21T00:00:00Z","relpermalink":"/publication/solid-harmonic-wavelet-scattering-predicting-quantum-molecular-energy-from-invariant-descriptors-of-3d-electronic-densities/","section":"publication","summary":"We introduce a solid harmonic wavelet scattering representation, which is invariant to rigid movements and stable to deformations, for regression and classification of 2D and 3D images. Solid harmonic wavelets are computed by multiplying solid harmonic functions with Gaussian windows dilated to different scales. Invariant scattering coefficients are obtained by cascading such wavelet transforms with the complex modulus nonlinearity. We study an application of solid harmonic scattering invariants to the estimation of quantum molecular energies, which are also invariant to rigid movements and stable with respect to deformations. We introduce a neural network with a multiplicative non-linearity for regression over scattering invariants to provide close to state of the art results over a database of organic molecules.","tags":["wavelets","quantum chemistry","multilinear regression","deep learning"],"title":"Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D  Electronic Densities","type":"publication"},{"authors":["Georgios Exarchakis","Jörg Lücke"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"a091e39a5660e0acf61ca89e64cb400b","permalink":"https://gexarcha.github.io/publication/discrete-sparse-coding/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/publication/discrete-sparse-coding/","section":"publication","summary":"Sparse coding algorithms with continuous latent variables have been the subject of a large number of studies. However, discrete latent spaces for sparse coding have been largely ignored. In this work, we study sparse coding with latents described by discrete instead of continuous prior distributions. We consider the general case in which the latents (while being sparse) can take on any value of a finite set of possible values and in which we learn the prior probability of any value from data. This approach can be applied to any data generated by discrete causes, and it can be applied as an approximation of continuous causes. As the prior probabilities are learned, the approach then allows for estimating the prior shape without assuming specific functional forms. To efficiently train the parameters of our probabilistic generative model, we apply a truncated expectation-maximization approach (expectation truncation) that we modify to work with a general discrete prior. We evaluate the performance of the algorithm by applying it to a variety of tasks: (1) we use artificial data to verify that the algorithm can recover the generating parameters from a random initialization, (2) use image patches of natural images and discuss the role of the prior for the extraction of image components, (3) use extracellular recordings of neurons to present a novel method of analysis for spiking neurons that includes an intuitive discretization strategy, and (4) apply the algorithm on the task of encoding audio waveforms of human speech. The diverse set of numerical experiments presented in this letter suggests that discrete sparse coding algorithms can scale efficiently to work with realistic data sets and provide novel statistical quantities to describe the structure of the data.","tags":["unsupervised learning","sparse coding","natural image statistics"],"title":"Discrete Sparse Coding","type":"publication"},{"authors":["Georgios Exarchakis"],"categories":null,"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480550400,"objectID":"7e27b597b00ee5efa7e04546f5e84660","permalink":"https://gexarcha.github.io/publication/probabilistic-models-for-invariant-representations-and-transformations/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/probabilistic-models-for-invariant-representations-and-transformations/","section":"publication","summary":"The central task of machine learning research is to extract regularities from data. These regularities are often subject to transformations that arise from the complexity of the process that generates the data. There has been a lot of effort towards creating data representations that are invariant to such transformations. However, most research towards learning invariances does not model the transformations explicitly. My research is focused towards modeling data in ways that separate their “content” from the potential “transformations” it undergoes. I primarily used a probabilistic generative framework due to its high expressive power and the belief that any potential representation will be subject to uncertainty. To model data content I focused on sparse coding techniques due to their ability to extract highly specialized dictionaries. I defined and implemented a discrete sparse coding model that models the presence/absence of a dictionary element subject to finite set of scaling transformations. I extended the discrete sparse coding model with an explicit representation for temporal shifts that learns time invariant representations for the data without loss of temporal alignment. In an attempt to create a more general model for data transformations, I defined a neural network that uses gating units to encode transformations from pairs of datapoints. Furthermore, I defined a non-linear dynamical system that expresses the dynamics in terms of a bilinear transformation that combines the previous state and a variable that encodes the transformation to generate the current state. In order to examine the behavior of these models in practice I tested them with on a variety of tasks. Almost always, I tested the models on recovering parameters from artificially generated data. Furthermore, I discovered interesting properties in the encoding of natural images, extra-cellular neural recordings, and audio data.","tags":["unsupervised learning","sparse coding","natural image statistics","invariant representations","neural networks"],"title":"Probabilistic Models for Invariant Representations and Transformations","type":"publication"},{"authors":["Zhenwen Dai","Georgios Exarchakis","Jörg Lücke"],"categories":null,"content":"","date":1386201600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1386201600,"objectID":"8f5b3a3b97cf33da1be69b774cb7ee9b","permalink":"https://gexarcha.github.io/publication/what-are-the-invariant-occlusive-components-of-image-patches-a-probabilistic-generative-approach/","publishdate":"2013-12-05T00:00:00Z","relpermalink":"/publication/what-are-the-invariant-occlusive-components-of-image-patches-a-probabilistic-generative-approach/","section":"publication","summary":"We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the first time apply a model with non-linear feature superposition and explicit position encoding. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We first investigated encodings learned by the model using artificial data with mutually occluding components. We find that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive fields associated with the model's hidden units. We find many Gabor-like or globular receptive fields as well as fields sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex.","tags":["unsupervised learning","invariant representations","natural image statistics","graphical models","machine learning"],"title":"What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach","type":"publication"},{"authors":["Roland Memisevic","Georgios Exarchakis"],"categories":null,"content":"","date":1370390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370390400,"objectID":"3f68e471714b3b3a48f8f7ddadbde467","permalink":"https://gexarcha.github.io/publication/learning-invariant-features-by-harnessing-the-aperture-problem/","publishdate":"2013-06-05T00:00:00Z","relpermalink":"/publication/learning-invariant-features-by-harnessing-the-aperture-problem/","section":"publication","summary":"The energy model is a simple, biologically inspired approach to extracting relationships between images in tasks like stereopsis and motion analysis. We discuss how adding an extra pooling layer to the energy model makes it possible to learn encodings of transformations that are mostly invariant with respect to image content, and to learn encodings of images that are mostly invariant with respect to the observed transformations. We show how this makes it possible to learn 3D pose-invariant features of objects by watching videos of the objects. We test our approach on a dataset of videos derived from the NORB dataset.","tags":["machine learning","deep learning","gated autoencoders","neural networks"],"title":"Learning invariant features by harnessing the aperture problem","type":"publication"},{"authors":["Georgios Exarchakis","Marc Henniges","Jörg Lücke"],"categories":null,"content":"","date":1317686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1317686400,"objectID":"67b807af61ee3caa5ad12548e6d513ac","permalink":"https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/","publishdate":"2011-10-04T00:00:00Z","relpermalink":"/publication/discrete-symmetric-priors-for-sparse-coding/","section":"publication","summary":"A standard model to explain the receptive fields of simple cells in the primary visual cortex is Sparse Coding (SC) [1]. However, the update equations used to train this model are not derivable in closed form. As a consequence, most state-of-the-art sparse coding versions use the MAP estimate for inference and training. Furthermore, it is not known if continuous hidden variables represent the best choice, e.g., for sparse coding as model for V1 processing. By using binary hidden variables, for instance, Binary Sparse Coding (BSC) [2], or [3], alternative priors with discrete hidden variables have been investigated in the past. The binary hidden space allows for analytically derivable update rules in closed-form and thus does not require a MAP estimation. However, in contrast, e.g., to Laplace priors, the Bernoulli distribution is not symmetric and its mean is not zero. To study the implications of discrete hidden variables independent of differences in prior symmetries, we, in this work, investigate a generative model with symmetrical and discrete prior distribution. Furthermore, a generative model with such a prior directly connects to recent sparse coding versions with hard-sparseness constraint (compare, e.g., [4]). As model for a discrete and symmetric prior, we use a multinomial distribution for hidden variables that can take on the values -1,0 and 1. In numerical experiments, we train the model using Expectation Truncation (ET) [5], a variational EM method which uses a preselection of hidden variables to increase learning efficiency. To show the effectiveness of the algorithm, we adjusted the linear bars test described in the BSC paper to fit our model. In the linear bars test the model was able to learn both the basis functions and the data noise. The linear bars test also provides considerable evidence that training the parameters using ET reduces the number of local optima. In experiments on more realistic data, we applied the algorithm to 50,000 large scale image patches (26x26 pixels) taken from the van Hateren image data base [6] and pre-processed with pseudo-whitening, using massive parallel computing. In this experiment, we obtained Gabor-like basis functions with similar properties as reported for receptive fields of V1 simple cells. We analyze the obtained Gabors and discuss differences and similarities to different sparse coding versions in the literature.","tags":["unsupervised learning","sparse coding","natural image statistics"],"title":"Discrete Symmetric Priors for Sparse Coding","type":"publication"}]