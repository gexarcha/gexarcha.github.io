<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.5.0"><meta name=author content="Georgios Exarchakis"><meta name=description content="A standard model to explain the receptive fields of simple cells in the primary visual cortex is Sparse Coding (SC) [1]. However, the update equations used to train this model are not derivable in closed form. As a consequence, most state-of-the-art sparse coding versions use the MAP estimate for inference and training. Furthermore, it is not known if continuous hidden variables represent the best choice, e.g., for sparse coding as model for V1 processing. By using binary hidden variables, for instance, Binary Sparse Coding (BSC) [2], or [3], alternative priors with discrete hidden variables have been investigated in the past. The binary hidden space allows for analytically derivable update rules in closed-form and thus does not require a MAP estimation. However, in contrast, e.g., to Laplace priors, the Bernoulli distribution is not symmetric and its mean is not zero. To study the implications of discrete hidden variables independent of differences in prior symmetries, we, in this work, investigate a generative model with symmetrical and discrete prior distribution. Furthermore, a generative model with such a prior directly connects to recent sparse coding versions with hard-sparseness constraint (compare, e.g., [4]). As model for a discrete and symmetric prior, we use a multinomial distribution for hidden variables that can take on the values -1,0 and 1. In numerical experiments, we train the model using Expectation Truncation (ET) [5], a variational EM method which uses a preselection of hidden variables to increase learning efficiency. To show the effectiveness of the algorithm, we adjusted the linear bars test described in the BSC paper to fit our model. In the linear bars test the model was able to learn both the basis functions and the data noise. The linear bars test also provides considerable evidence that training the parameters using ET reduces the number of local optima. In experiments on more realistic data, we applied the algorithm to 50,000 large scale image patches (26x26 pixels) taken from the van Hateren image data base [6] and pre-processed with pseudo-whitening, using massive parallel computing. In this experiment, we obtained Gabor-like basis functions with similar properties as reported for receptive fields of V1 simple cells. We analyze the obtained Gabors and discuss differences and similarities to different sparse coding versions in the literature."><link rel=alternate hreflang=en-us href=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/><meta name=theme-color content="#2962ff"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.min.82936fefd6d50005ef2d16e485a17a90.css><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-125269659-1','auto');ga('require','eventTracker');ga('require','outboundLinkTracker');ga('require','urlChangeTracker');ga('send','pageview');</script><script async src=https://www.google-analytics.com/analytics.js></script><script async src=https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin=anonymous></script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/img/icon-32.png><link rel=apple-touch-icon type=image/png href=/img/icon-192.png><link rel=canonical href=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@gexarcha"><meta property="twitter:creator" content="@gexarcha"><meta property="og:site_name" content="Georgios Exarchakis"><meta property="og:url" content="https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/"><meta property="og:title" content="Discrete Symmetric Priors for Sparse Coding | Georgios Exarchakis"><meta property="og:description" content="A standard model to explain the receptive fields of simple cells in the primary visual cortex is Sparse Coding (SC) [1]. However, the update equations used to train this model are not derivable in closed form. As a consequence, most state-of-the-art sparse coding versions use the MAP estimate for inference and training. Furthermore, it is not known if continuous hidden variables represent the best choice, e.g., for sparse coding as model for V1 processing. By using binary hidden variables, for instance, Binary Sparse Coding (BSC) [2], or [3], alternative priors with discrete hidden variables have been investigated in the past. The binary hidden space allows for analytically derivable update rules in closed-form and thus does not require a MAP estimation. However, in contrast, e.g., to Laplace priors, the Bernoulli distribution is not symmetric and its mean is not zero. To study the implications of discrete hidden variables independent of differences in prior symmetries, we, in this work, investigate a generative model with symmetrical and discrete prior distribution. Furthermore, a generative model with such a prior directly connects to recent sparse coding versions with hard-sparseness constraint (compare, e.g., [4]). As model for a discrete and symmetric prior, we use a multinomial distribution for hidden variables that can take on the values -1,0 and 1. In numerical experiments, we train the model using Expectation Truncation (ET) [5], a variational EM method which uses a preselection of hidden variables to increase learning efficiency. To show the effectiveness of the algorithm, we adjusted the linear bars test described in the BSC paper to fit our model. In the linear bars test the model was able to learn both the basis functions and the data noise. The linear bars test also provides considerable evidence that training the parameters using ET reduces the number of local optima. In experiments on more realistic data, we applied the algorithm to 50,000 large scale image patches (26x26 pixels) taken from the van Hateren image data base [6] and pre-processed with pseudo-whitening, using massive parallel computing. In this experiment, we obtained Gabor-like basis functions with similar properties as reported for receptive fields of V1 simple cells. We analyze the obtained Gabors and discuss differences and similarities to different sparse coding versions in the literature."><meta property="og:image" content="https://gexarcha.github.io/img/George%20Exarchakis.jpg"><meta property="twitter:image" content="https://gexarcha.github.io/img/George%20Exarchakis.jpg"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2011-10-04T00:00:00&#43;00:00"><meta property="article:modified_time" content="2011-10-04T00:00:00&#43;00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/"},"headline":"Discrete Symmetric Priors for Sparse Coding","datePublished":"2011-10-04T00:00:00Z","dateModified":"2011-10-04T00:00:00Z","author":{"@type":"Person","name":"Georgios Exarchakis"},"publisher":{"@type":"Organization","name":"Georgios Exarchakis","logo":{"@type":"ImageObject","url":"https://gexarcha.github.io/img/icon-512.png"}},"description":"A standard model to explain the receptive fields of simple cells in the primary visual cortex is Sparse Coding (SC) [1]. However, the update equations used to train this model are not derivable in closed form. As a consequence, most state-of-the-art sparse coding versions use the MAP estimate for inference and training. Furthermore, it is not known if continuous hidden variables represent the best choice, e.g., for sparse coding as model for V1 processing. By using binary hidden variables, for instance, Binary Sparse Coding (BSC) [2], or [3], alternative priors with discrete hidden variables have been investigated in the past. The binary hidden space allows for analytically derivable update rules in closed-form and thus does not require a MAP estimation. However, in contrast, e.g., to Laplace priors, the Bernoulli distribution is not symmetric and its mean is not zero. To study the implications of discrete hidden variables independent of differences in prior symmetries, we, in this work, investigate a generative model with symmetrical and discrete prior distribution. Furthermore, a generative model with such a prior directly connects to recent sparse coding versions with hard-sparseness constraint (compare, e.g., [4]). As model for a discrete and symmetric prior, we use a multinomial distribution for hidden variables that can take on the values -1,0 and 1. In numerical experiments, we train the model using Expectation Truncation (ET) [5], a variational EM method which uses a preselection of hidden variables to increase learning efficiency. To show the effectiveness of the algorithm, we adjusted the linear bars test described in the BSC paper to fit our model. In the linear bars test the model was able to learn both the basis functions and the data noise. The linear bars test also provides considerable evidence that training the parameters using ET reduces the number of local optima. In experiments on more realistic data, we applied the algorithm to 50,000 large scale image patches (26x26 pixels) taken from the van Hateren image data base [6] and pre-processed with pseudo-whitening, using massive parallel computing. In this experiment, we obtained Gabor-like basis functions with similar properties as reported for receptive fields of V1 simple cells. We analyze the obtained Gabors and discuss differences and similarities to different sparse coding versions in the literature."}</script><title>Discrete Symmetric Priors for Sparse Coding | Georgios Exarchakis</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id=navbar-main><div class=container><a class=navbar-brand href=/>Georgios Exarchakis</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav mr-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><div class=pub><div class="article-container pt-3"><h1>Discrete Symmetric Priors for Sparse Coding</h1><div class=article-metadata><div><span><a href=/authors/georgios-exarchakis/>Georgios Exarchakis</a></span>, <span><a href=/authors/marc-henniges/>Marc Henniges</a></span>, <span>Jörg Lücke</span></div><span class=article-date>October 2011</span>
<span class=middot-divider></span><a href=/publication/discrete-symmetric-priors-for-sparse-coding/#disqus_thread></a><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/&amp;text=Discrete%20Symmetric%20Priors%20for%20Sparse%20Coding" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/&amp;t=Discrete%20Symmetric%20Priors%20for%20Sparse%20Coding" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Discrete%20Symmetric%20Priors%20for%20Sparse%20Coding&amp;body=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/&amp;title=Discrete%20Symmetric%20Priors%20for%20Sparse%20Coding" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://web.whatsapp.com/send?text=Discrete%20Symmetric%20Priors%20for%20Sparse%20Coding%20https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://gexarcha.github.io/publication/discrete-symmetric-priors-for-sparse-coding/&amp;title=Discrete%20Symmetric%20Priors%20for%20Sparse%20Coding" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://link.springer.com/chapter/10.1007/978-3-642-28551-6_26 target=_blank rel=noopener>PDF</a>
<button type=button class="btn btn-outline-primary my-1 mr-1 js-cite-modal" data-filename=/publication/discrete-symmetric-priors-for-sparse-coding/cite.bib>
Cite</button></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>A standard model to explain the receptive fields of simple cells in the primary visual cortex is Sparse Coding (SC) [1]. However, the update equations used to train this model are not derivable in closed form. As a consequence, most state-of-the-art sparse coding versions use the MAP estimate for inference and training. Furthermore, it is not known if continuous hidden variables represent the best choice, e.g., for sparse coding as model for V1 processing. By using binary hidden variables, for instance, Binary Sparse Coding (BSC) [2], or [3], alternative priors with discrete hidden variables have been investigated in the past. The binary hidden space allows for analytically derivable update rules in closed-form and thus does not require a MAP estimation. However, in contrast, e.g., to Laplace priors, the Bernoulli distribution is not symmetric and its mean is not zero. To study the implications of discrete hidden variables independent of differences in prior symmetries, we, in this work, investigate a generative model with symmetrical and discrete prior distribution. Furthermore, a generative model with such a prior directly connects to recent sparse coding versions with hard-sparseness constraint (compare, e.g., [4]). As model for a discrete and symmetric prior, we use a multinomial distribution for hidden variables that can take on the values -1,0 and 1. In numerical experiments, we train the model using Expectation Truncation (ET) [5], a variational EM method which uses a preselection of hidden variables to increase learning efficiency. To show the effectiveness of the algorithm, we adjusted the linear bars test described in the BSC paper to fit our model. In the linear bars test the model was able to learn both the basis functions and the data noise. The linear bars test also provides considerable evidence that training the parameters using ET reduces the number of local optima. In experiments on more realistic data, we applied the algorithm to 50,000 large scale image patches (26x26 pixels) taken from the van Hateren image data base [6] and pre-processed with pseudo-whitening, using massive parallel computing. In this experiment, we obtained Gabor-like basis functions with similar properties as reported for receptive fields of V1 simple cells. We analyze the obtained Gabors and discuss differences and similarities to different sparse coding versions in the literature.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">In <em>Bernstein Conference</em> 2011.</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=article-tags><a class="badge badge-light" href=/tags/unsupervised-learning/>unsupervised learning</a>
<a class="badge badge-light" href=/tags/sparse-coding/>sparse coding</a>
<a class="badge badge-light" href=/tags/natural-image-statistics/>natural image statistics</a></div><div class="media author-card"><div class=media-body><h5 class=card-title><a href=/authors/georgios-exarchakis/></a></h5><ul class=network-icon aria-hidden=true></ul></div></div><section id=comments><div id=disqus_thread></div><script>let disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return;}
var d=document,s=d.createElement('script');s.async=true;s.src='https://'+"https-gexarcha-github-io"+'.disqus.com/embed.js';s.setAttribute('data-timestamp',+new Date());(d.head||d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></div><script src=/js/mathjax-config.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin=anonymous async></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin=anonymous></script><script>const search_index_filename="/index.json";const i18n={'placeholder':"Search...",'results':"results found",'no_results':"No results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script id=dsq-count-scr src=https://https-gexarcha-github-io.disqus.com/count.js async></script><script src=/js/academic.min.130521ecfc6f534c52c158217bbff718.js></script><div class=container><footer class=site-footer></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&times;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>